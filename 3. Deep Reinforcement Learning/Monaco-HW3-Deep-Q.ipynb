{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd581c9-b9e4-4cb0-bd4d-dcc781d5d906",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "### A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "\n",
    "### Saverio Monaco\n",
    "##### MAT: 2012264\n",
    "\n",
    "# Homework 3 - Deep Reinforcement Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c01cf3-ee6a-4aa3-a1f9-5d22c6918a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### IMPORTS ####\n",
    "#################\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "from collections import deque # fixed size FIFO list\n",
    "\n",
    "# Deep Learning Stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import Monitor\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import base64\n",
    "\n",
    "# Other\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Automatic tuning tool\n",
    "import optuna\n",
    "\n",
    "# Second set\n",
    "import flappy_bird_gym\n",
    "import time\n",
    "from scipy import ndimage # rotate image for display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2daba1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2684f8-fc99-43da-b1fc-37cf70a4eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "742bba9d-45b6-4857-b692-3dc61d18fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### CLASSES ####\n",
    "#################\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''\n",
    "    To perform experience replay.\n",
    "    We will draw uniformly at random from the pool of stored sample to learn.\n",
    "    Thi avoids (temporal) correlation between consecutive learning instances.\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        ''' Initialize a deque with maximum capacity maxlen. '''\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        ''' Add a new sample to the deque, removes the oldest one if it is already full. '''\n",
    "        # Add the tuple (state, action, next_state, reward) to the queue\n",
    "        self.memory.append( (state, action, next_state, reward) )\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' Randomly select \"batch_size\" samples '''\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' Return the number of samples currently stored in the memory '''\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    ''' \n",
    "    Network for policy network and target network \n",
    "    state_space_dim:  (INPUT)  dimension of state space (e.g pixels in a image)\n",
    "    action_space_dim: (OUTPUT) dimension of action space (e.g go left, go right)\n",
    "    '''\n",
    "    def __init__(self, DQN_state_space_dim, DQN_action_space_dim):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.sdim = DQN_state_space_dim\n",
    "        self.adim = DQN_action_space_dim\n",
    "            \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.sdim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,self.adim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "#\n",
    "#          |---------> [Prediction Network (DQN)]--------\n",
    "#          |                |                            \\\n",
    "# [INPUT]--|                | Parameter update            \\___Loss\n",
    "#          |               \\/                            /\n",
    "#          |---------> [Target Network (DQN)]------------\n",
    "#\n",
    "class FullQNets(nn.Module):\n",
    "    ''' \n",
    "    Handles all the networks, environments, and others\n",
    "    '''\n",
    "    def __init__(self, envname):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.envname = envname\n",
    "        \n",
    "        tempenv = gym.make(self.envname) \n",
    "            \n",
    "        self.state_space_dim = tempenv.observation_space.shape[0]\n",
    "        \n",
    "        if self.envname == 'BipedalWalkerHardcore-v3':\n",
    "            self.action_space_dim = 4 # Box attribute in BipedalWalker are different\n",
    "        else:\n",
    "            self.action_space_dim = tempenv.action_space.n\n",
    "        \n",
    "        self.policy_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def choose_action_epsilon_greedy(self, state, epsilon):\n",
    "        self.policy_net.eval()\n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device) # Convert the state to tensor\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "\n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "        \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def choose_action_softmax(self, state, temperature):\n",
    "        self.policy_net.to(device)\n",
    "        if temperature < 0:\n",
    "            raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "        # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "        if temperature == 0:\n",
    "            return self.choose_action_epsilon_greedy(state, 0)\n",
    "    \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Apply softmax with temp\n",
    "        temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "        softmax_out = nn.functional.softmax(net_out / temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "        # Sample the action using softmax output as mass pdf\n",
    "        all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "        action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def update_step(self, replay_mem, gamma, optimizer, loss_fn, batch_size):        \n",
    "        self.policy_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        # Sample the data from the replay memory\n",
    "        batch = replay_mem.sample(batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Create tensors for each element of the batch\n",
    "        states      = torch.tensor([s[0] for s in batch], dtype=torch.float32, device=device)\n",
    "        actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64, device=device)\n",
    "        rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "        non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32, device=device) # the next state can be None if the game has ended\n",
    "        non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Compute all the Q values (forward pass)\n",
    "        self.policy_net.train()\n",
    "        q_values = self.policy_net(states)\n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            q_values_target = self.target_net(non_final_next_states)\n",
    "        next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def play_a_game(self, show = True, debug = False):\n",
    "        # Initialize the Gym environment\n",
    "        env = gym.make(self.envname) \n",
    "        \n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "        # Go on until the pole falls off or the score reach 490\n",
    "        while not done:\n",
    "            # Choose the best action (temperature 0)\n",
    "            action, q_values = self.choose_action_softmax(state, temperature=0)\n",
    "            if debug:\n",
    "                print('action', action)\n",
    "                print('state', state)\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Visually render the environment\n",
    "            if show:\n",
    "                env.render()\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += reward \n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "            # Check if the episode ended (the pole fell down)\n",
    "        # Print the final score\n",
    "        if show:\n",
    "            print(f\"SCORE: {score}\") \n",
    "        env.close()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def training_loop(self, max_epoch, loss_fn, optimizer_fn, initial_value=5, gamma = 0.97, \n",
    "                 replay_memory_capacity = 10000, lr = 1e-2, reg_opt = 1e-5, \n",
    "                 target_net_update_steps = 10, bad_state_penalty = 0, test_policy = False,\n",
    "                 batch_size = 128, min_samples_for_training = 1000, render=False, optunaprint = False, maxplayers = 0,\n",
    "                 optuna_epoch_limit = False):\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        gamma: gamma parameter for the long term reward\n",
    "        replay_memory_capacity: Replay memory capacity\n",
    "        lr: Optimizer learning rate\n",
    "        loss_fn: Loss function\n",
    "        optimizer_fn: Optimizer function\n",
    "        target_net_update_steps: Number of episodes to wait before updating the target network\n",
    "        batch_size: Number of samples to take from the replay memory for each update\n",
    "        bad_state_penalty: Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "        min_samples_for_training: Minimum samples in the replay memory to enable the training\n",
    "        test_policy: if True, for every target_net_update_steps steps, it will play 10 games following its policy (T=0)\n",
    "                     and if it obtains the maximum score (500) it will exit\n",
    "                     if is int > 0 it will do the same playing test_policy number of games\n",
    "        '''\n",
    "        env = gym.make(self.envname)\n",
    "        optimizer = optimizer_fn(self.policy_net.parameters(), lr=lr, weight_decay=reg_opt)\n",
    "        has_learned = False # Boolean variable, if test_policy == True and has obtained the max score on the test games\n",
    "                            # it will exit learning\n",
    "        # We compute the exponential decay in such a way the shape of the exploration \n",
    "        # profile does not depend on the number of iterations\n",
    "        exp_decay = np.exp(-np.log(initial_value) / max_epoch * 6) \n",
    "        exploration_profile = [initial_value * (exp_decay ** i) for i in range(max_epoch)]\n",
    "        \n",
    "        ### Initialize the replay memory\n",
    "        replay_mem = ReplayMemory(replay_memory_capacity)   \n",
    "        \n",
    "        env.seed(0)\n",
    "        policy_num = 0\n",
    "        progress = tqdm(exploration_profile)\n",
    "        #print('n iterations:', len(exploration_profile) )\n",
    "        for episode_num, tau in enumerate(progress):\n",
    "            # Reset the environment and get the initial state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "            score = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            # Go on until the pole falls off\n",
    "            while not done:\n",
    "                # Choose the action following the policy\n",
    "                action, q_values = self.choose_action_softmax(state, temperature=tau)\n",
    "                \n",
    "                # Apply the action and get the next state, the reward and a flag \"done\" \n",
    "                # that is True if the game is ended\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # We apply a (linear) penalty when the cart is far from center\n",
    "                pos_weight = 1\n",
    "                reward = reward - pos_weight * np.abs(state[0])\n",
    "                \n",
    "                # Update the final score (+1 for each step)\n",
    "                score += 1\n",
    "                \n",
    "                # Update the replay memory\n",
    "                replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "                # Update the network\n",
    "                if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                    optimizer = self.update_step(replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "                \n",
    "                if render:\n",
    "                    # Visually render the environment (disable to speed up the training)\n",
    "                    env.render()\n",
    "\n",
    "                # Set the current state for the next iteration\n",
    "                state = next_state\n",
    "\n",
    "            # Update the target network every target_net_update_steps episodes\n",
    "            if episode_num % target_net_update_steps == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "                \n",
    "                # If test_policy is True, we check every target_net_update_setps if the net has learned.+\n",
    "                # the net has learnet if it can obtain the maximum score 10 times in a row following its policy (with T=0)\n",
    "                # or test_policy times in a row if is int\n",
    "                if test_policy:\n",
    "                    policy_num = policy_num + 1\n",
    "                    if type(test_policy)==int:\n",
    "                        ngames = test_policy\n",
    "                    else:\n",
    "                        ngames = 10\n",
    "                        \n",
    "                    test_score = 0\n",
    "                    for game in range(ngames):\n",
    "                        test_score = test_score + self.play_a_game(show=False)\n",
    "                    test_score = test_score/ngames\n",
    "                    if not type(optunaprint)==int:\n",
    "                        print('Policy {:d} | Mean SCORE: {:.2f} '.format(policy_num,test_score) )\n",
    "                    if test_score >= 500:\n",
    "                        has_learned = True\n",
    "            # Print the final score\n",
    "            if optunaprint:\n",
    "                progress.set_description(\"{:d}/{:d} :: SCORE: {:d} - T: {:.4f}\".format(optunaprint,maxplayers, score,tau))\n",
    "            else:\n",
    "                progress.set_description(\"SCORE: {:d} - T: {:.4f}\".format(score,tau))\n",
    "            \n",
    "            if has_learned == True:\n",
    "                if not optunaprint:\n",
    "                    print('Learning completed at epoch: {:d}'.format(episode_num))\n",
    "                env.close()\n",
    "                return episode_num\n",
    "            \n",
    "            # For speeding up the training for the optuna gridsearch, we might want to kill the learning \n",
    "            # of models that are taking longer than the best one\n",
    "            if type(optuna_epoch_limit) == int:\n",
    "                if episode_num >= optuna_epoch_limit:\n",
    "                    env.close()\n",
    "                    return episode_num\n",
    "\n",
    "        env.close()\n",
    "        return max_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceacdc24-780d-4914-bf84-07e0393f0213",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b57674-066a-4182-9cd2-dd610a7e5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_qlearning(ntrials = 200, nepochs = 1000):\n",
    "    ''' Function for Optuna hyperparameter tuning '''\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='minimize')\n",
    "    \n",
    "    optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "    global player\n",
    "    player = 1\n",
    "    global epochs\n",
    "    epochs = nepochs\n",
    "    def callback(study, trial):\n",
    "        ''' Callback function, saves the best model. '''\n",
    "        global best_model\n",
    "        # If the best model result is the current one...\n",
    "        if study.best_trial == trial:\n",
    "            # Saves the best model to best_model\n",
    "            best_model = optmodel\n",
    "    \n",
    "    def optuna_train(trial):\n",
    "        ''' Main function for optuna. '''\n",
    "        cfg = {\n",
    "          'reg': trial.suggest_loguniform('reg',1e-5, 1e-3),\n",
    "          'lr' : trial.suggest_loguniform('lr', 1e-4, 1e-3),\n",
    "          'optimizer': trial.suggest_categorical('optimizer',list(range(len(optimizers)))),\n",
    "          'gamma' : trial.suggest_uniform('gamma', .95, 1),\n",
    "          #'penality' : trial.suggest_categorical('penality', [0]),\n",
    "          'init_v' : trial.suggest_loguniform('init_v', 1,10)\n",
    "        }\n",
    "        \n",
    "        global player\n",
    "        global optmodel\n",
    "        global epochs\n",
    "        optmodel =  FullQNets('CartPole-v1')\n",
    "        optmodel.to(device)\n",
    "        \n",
    "        learning_epochs = optmodel.training_loop(nepochs, nn.SmoothL1Loss(), optimizers[cfg['optimizer']], lr=cfg['lr'], test_policy=10,\n",
    "                                                 reg_opt = cfg['reg'], gamma=cfg['gamma'], initial_value=cfg['init_v'],\n",
    "                                                 bad_state_penalty = 0, optunaprint=player, maxplayers = ntrials,\n",
    "                                                 optuna_epoch_limit = epochs);\n",
    "        if learning_epochs < epochs:\n",
    "            epochs = learning_epochs\n",
    "        \n",
    "        player = player + 1\n",
    "        return learning_epochs\n",
    "    \n",
    "    study.optimize(optuna_train, n_trials=ntrials, callbacks=[callback])\n",
    "    \n",
    "    optuna_best_params = {\n",
    "        'reg'   : study.best_params['reg'],\n",
    "        'lr'    : study.best_params['lr'],\n",
    "        'optimizer'   : optimizers[study.best_params['optimizer']],\n",
    "        'gamma' : study.best_params['gamma'],\n",
    "        #'penality' : study.best_params['penality'],\n",
    "        'init_v' : study.best_params['init_v']\n",
    "    }\n",
    "    \n",
    "    return study, optuna_best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874df46e-11ce-425e-b955-d2f77c1b8540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study, optuna_best_params, best_player = optuna_qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ddbc5-625f-461c-8c28-fe63e1242022",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_player.play_a_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c4bdf-4f4c-4826-879a-1998aa259fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study, target_name='Episodes to learn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9724f1e-c0c2-4fef-9104-12e4e4d58664",
   "metadata": {},
   "source": [
    "## FlappyBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5dd5227d-ee40-482b-a8e6-ec3d1ae915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### CLASSES ####\n",
    "#################\n",
    "class ImagDQN(nn.Module):\n",
    "    ''' \n",
    "    Network for policy network and target network \n",
    "    state_space_dim:  (INPUT)  dimension of state space (e.g pixels in a image)\n",
    "    action_space_dim: (OUTPUT) dimension of action space (e.g go left, go right)\n",
    "    '''\n",
    "    def __init__(self, DQN_state_space_dim, DQN_action_space_dim):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.sdim = DQN_state_space_dim\n",
    "        self.adim = DQN_action_space_dim\n",
    "            \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.sdim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,self.adim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x, batch_size = False):\n",
    "        if not batch_size:\n",
    "            x = torch.flatten(x)\n",
    "        else:\n",
    "            x= torch.reshape(x, (batch_size, 560))\n",
    "        return self.linear(x)\n",
    "#\n",
    "#          |---------> [Prediction Network (DQN)]--------\n",
    "#          |                |                            \\\n",
    "# [INPUT]--|                | Parameter update            \\___Loss\n",
    "#          |               \\/                            /\n",
    "#          |---------> [Target Network (DQN)]------------\n",
    "#\n",
    "class Flappy(nn.Module):\n",
    "    ''' \n",
    "    Handles all the networks, environments, and others\n",
    "    '''\n",
    "    def __init__(self, rgb = False, verbose = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not rgb:\n",
    "            tempenv = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "            self.rgb = False\n",
    "            self.state_space_dim = tempenv.observation_space.shape[0]\n",
    "        else:\n",
    "            tempenv = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "            self.rgb = True\n",
    "            self.state_space_dim = 560\n",
    "        \n",
    "        \n",
    "        self.action_space_dim = tempenv.action_space.n\n",
    "        \n",
    "        if verbose:\n",
    "            print('Environment informations:')\n",
    "            print('  observation space:', tempenv.observation_space)\n",
    "            print('  action space     :', tempenv.action_space)\n",
    "            print('  nets input parameters :', self.state_space_dim)\n",
    "            print('  nets output parameters:', self.action_space_dim)\n",
    "        \n",
    "        if not rgb:\n",
    "            self.policy_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "            self.target_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        else:\n",
    "            self.policy_net = ImagDQN(self.state_space_dim, self.action_space_dim)\n",
    "            self.target_net = ImagDQN(self.state_space_dim, self.action_space_dim)\n",
    "            \n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def compress_input_image(self, state_np):\n",
    "        pooled = max_pool(state_np[:,:,2][70:280,0:400], 10)\n",
    "        #pooled_col1 = pooled[:1,:].max(axis=0)\n",
    "        pooled_col2 = pooled[12:16,:].min(axis=0)\n",
    "        pooled_col3 = pooled[16:,:].min(axis=0)\n",
    "        \n",
    "        pooled2 = np.vstack((np.vstack(( pooled[0:12,:], pooled_col2 )), pooled_col3)) \n",
    "    \n",
    "        # assign background color to 0\n",
    "        pooled2[pooled2 == 200] = 0\n",
    "        pooled2[pooled2 > 200] = 5 # the bird\n",
    "        pooled2[pooled2 > 5] = 1 # the pipes\n",
    "        \n",
    "        return pooled2\n",
    "    def choose_action_epsilon_greedy(self, state, epsilon):\n",
    "        self.policy_net.eval()\n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device) # Convert the state to tensor\n",
    "            \n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "\n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "        \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def choose_action_softmax(self, state, temperature):\n",
    "        self.policy_net.to(device)\n",
    "        if temperature < 0:\n",
    "            raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "        # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "        if temperature == 0:\n",
    "            return self.choose_action_epsilon_greedy(state, 0)\n",
    "    \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Apply softmax with temp\n",
    "        temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "        softmax_out = nn.functional.softmax(net_out / temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "        # Sample the action using softmax output as mass pdf\n",
    "        all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "        action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def update_step(self, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        #print('entering')\n",
    "        self.policy_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        # Sample the data from the replay memory\n",
    "        batch = replay_mem.sample(batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Create tensors for each element of the batch\n",
    "        states      = torch.tensor([s[0] for s in batch], dtype=torch.float32, device=device)\n",
    "        actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64, device=device)\n",
    "        rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "        non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32, device=device) # the next state can be None if the game has ended\n",
    "        non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Compute all the Q values (forward pass)\n",
    "        self.policy_net.train()\n",
    "        if self.rgb:\n",
    "            q_values = self.policy_net(states, batch_size)\n",
    "        else:\n",
    "            q_values = self.policy_net(states)\n",
    "            \n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            if self.rgb:\n",
    "                q_values_target = self.target_net(non_final_next_states, batch_size)\n",
    "            else:   \n",
    "                q_values_target = self.target_net(non_final_next_states)\n",
    "                \n",
    "        next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print('exiting')\n",
    "        return optimizer\n",
    "    \n",
    "    def play_a_game(self, show = False, debug = False):\n",
    "        # Initialize the Gym environmentif not rgb:\n",
    "        if self.rgb:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "        else:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "        \n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        if self.rgb:\n",
    "            state = self.compress_input_image(state)\n",
    "        \n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "        # Go on until the pole falls off or the score reach 490\n",
    "        while not done:\n",
    "            # Choose the best action (temperature 0)\n",
    "            action, q_values = self.choose_action_softmax(state, temperature=0)\n",
    "            \n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if self.rgb:\n",
    "                next_state = self.compress_input_image(next_state)\n",
    "            # Visually render the environment\n",
    "            if show:\n",
    "                env.render()\n",
    "                time.sleep(1 / 30)  # FPS\n",
    "                if debug and (self.rgb == True):\n",
    "                    clear_output(wait=True)\n",
    "                    plt.imshow(ndimage.rotate(np.flip(state,0), 270), cmap='gray')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    #if 5 in state[:2,:]:\n",
    "                        #yscreen = np.shape(state[:2,:])[1]\n",
    "                        #print(np.abs(state[:2,:].argmax() - yscreen/2)/(yscreen/2))\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += reward \n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "            # Check if the episode ended (the pole fell down)\n",
    "            #done = True\n",
    "        # Print the final score\n",
    "        if show:\n",
    "            print(f\"SCORE: {score}\") \n",
    "        env.close()\n",
    "        \n",
    "        return score, state\n",
    "    \n",
    "    def training_loop(self, max_epoch, loss_fn, optimizer_fn, initial_value=5, gamma = 0.97, \n",
    "                 replay_memory_capacity = 10000, lr = 1e-2, reg_opt = 1e-5, \n",
    "                 target_net_update_steps = 10, bad_state_penalty = 0, test_policy = False,\n",
    "                 batch_size = 128, min_samples_for_training = 1000, render=False, optunaprint = False, maxplayers = 0,\n",
    "                 optuna_epoch_limit = False, maxscore = 2500):\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        gamma: gamma parameter for the long term reward\n",
    "        replay_memory_capacity: Replay memory capacity\n",
    "        lr: Optimizer learning rate\n",
    "        loss_fn: Loss function\n",
    "        optimizer_fn: Optimizer function\n",
    "        target_net_update_steps: Number of episodes to wait before updating the target network\n",
    "        batch_size: Number of samples to take from the replay memory for each update\n",
    "        bad_state_penalty: Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "        min_samples_for_training: Minimum samples in the replay memory to enable the training\n",
    "        test_policy: if True, for every target_net_update_steps steps, it will play 10 games following its policy (T=0)\n",
    "                     and if it obtains the maximum score (500) it will exit\n",
    "                     if is int > 0 it will do the same playing test_policy number of games\n",
    "        '''\n",
    "        \n",
    "        if self.rgb:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "        else:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "            \n",
    "        optimizer = optimizer_fn(self.policy_net.parameters(), lr=lr, weight_decay=reg_opt)\n",
    "        has_learned = False # Boolean variable, if test_policy == True and has obtained the max score on the test games\n",
    "                            # it will exit learning\n",
    "        # We compute the exponential decay in such a way the shape of the exploration \n",
    "        # profile does not depend on the number of iterations\n",
    "        exp_decay = np.exp(-np.log(initial_value) / max_epoch * 6) \n",
    "        exploration_profile = [initial_value * (exp_decay ** i) for i in range(max_epoch)]\n",
    "        \n",
    "        ### Initialize the replay memory\n",
    "        replay_mem = ReplayMemory(replay_memory_capacity)   \n",
    "        \n",
    "        env.seed(0)\n",
    "        policy_num = 0\n",
    "        progress = tqdm(exploration_profile)\n",
    "        #print('n iterations:', len(exploration_profile) )\n",
    "        for episode_num, tau in enumerate(progress):\n",
    "            # Reset the environment and get the initial state\n",
    "            state = env.reset()\n",
    "            if self.rgb:\n",
    "                state = self.compress_input_image(state)\n",
    "                \n",
    "            # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "            score = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            pregame_actions = 0\n",
    "            pregame_nonactions = 0\n",
    "            # Go on until the pole falls off\n",
    "            while not done:\n",
    "                # Choose the action following the policy\n",
    "                action, q_values = self.choose_action_softmax(state, temperature=tau)\n",
    "                \n",
    "                # Apply the action and get the next state, the reward and a flag \"done\" \n",
    "                # that is True if the game is ended\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                if self.rgb:\n",
    "                    next_state = self.compress_input_image(next_state)\n",
    "                    \n",
    "                # We apply a (linear) penalty when the cart is far from center\n",
    "                \n",
    "                if not self.rgb:\n",
    "                    pos_weight = 5\n",
    "                    reward = reward - pos_weight * np.abs(state[1])\n",
    "                else:\n",
    "                    if not 5 in state[:2,:]:\n",
    "                        pos_weight = 5\n",
    "                        reward = reward - pos_weight\n",
    "                    \n",
    "                    #yscreen = np.shape(state[:2,:])[1]\n",
    "                    #if 5 in state[:2,:]:\n",
    "                    #    reward = reward - pos_weight*np.abs(state[:2,:].argmax() - yscreen/2)/(yscreen/2)\n",
    "                    #else:\n",
    "                    #    reward = reward - pos_weight\n",
    "                \n",
    "                # Update the final score (+1 for each step)\n",
    "                score += 1\n",
    "                \n",
    "                # Update the replay memory\n",
    "                replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "                # Update the network\n",
    "                if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                    optimizer = self.update_step(replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "                \n",
    "                if render:\n",
    "                    # Visually render the environment (disable to speed up the training)\n",
    "                    env.render()\n",
    "\n",
    "                # Set the current state for the next iteration\n",
    "                state = next_state\n",
    "            \n",
    "            if score>=maxscore:\n",
    "                env.close()\n",
    "                return episode_num\n",
    "            \n",
    "            # Update the target network every target_net_update_steps episodes\n",
    "            if episode_num % target_net_update_steps == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "                \n",
    "                # If test_policy is True, we check every target_net_update_setps if the net has learned.+\n",
    "                # the net has learnet if it can obtain the maximum score 10 times in a row following its policy (with T=0)\n",
    "                # or test_policy times in a row if is int\n",
    "                if test_policy:\n",
    "                    policy_num = policy_num + 1\n",
    "                    if type(test_policy)==int:\n",
    "                        ngames = test_policy\n",
    "                    else:\n",
    "                        ngames = 10\n",
    "                        \n",
    "                    test_score = 0\n",
    "                    for game in range(ngames):\n",
    "                        test_score = test_score + self.play_a_game(show=False)[0]\n",
    "                    test_score = test_score/ngames\n",
    "                    if not type(optunaprint)==int:\n",
    "                        print('Policy {:d} | Mean SCORE: {:.2f} '.format(policy_num,test_score) )\n",
    "                    if test_score >= maxscore:\n",
    "                        has_learned = True\n",
    "            # Print the final score\n",
    "            if optunaprint:\n",
    "                progress.set_description(\"{:d}/{:d} :: SCORE: {:d} - T: {:.4f}\".format(optunaprint,maxplayers, score,tau))\n",
    "            else:\n",
    "                progress.set_description(\"SCORE: {:d} - T: {:.4f}\".format(score,tau))\n",
    "            \n",
    "            if has_learned == True:\n",
    "                if not optunaprint:\n",
    "                    print('Learning completed at epoch: {:d}'.format(episode_num))\n",
    "                env.close()\n",
    "                return episode_num\n",
    "            \n",
    "            # For speeding up the training for the optuna gridsearch, we might want to kill the learning \n",
    "            # of models that are taking longer than the best one\n",
    "            if type(optuna_epoch_limit) == int:\n",
    "                if episode_num >= optuna_epoch_limit:\n",
    "                    env.close()\n",
    "                    return episode_num\n",
    "\n",
    "        env.close()\n",
    "        return max_epoch\n",
    "    \n",
    "def optuna_flappy(ntrials = 20, nepochs = 1000):\n",
    "    ''' Function for Optuna hyperparameter tuning '''\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='minimize')\n",
    "    \n",
    "    optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "    global player\n",
    "    player = 1\n",
    "    global epochs\n",
    "    epochs = nepochs\n",
    "    def callback(study, trial):\n",
    "        ''' Callback function, saves the best model. '''\n",
    "        global best_model\n",
    "        # If the best model result is the current one...\n",
    "        if study.best_trial == trial:\n",
    "            # Saves the best model to best_model\n",
    "            best_model = optmodel\n",
    "    \n",
    "    def optuna_train(trial):\n",
    "        ''' Main function for optuna. '''\n",
    "        cfg = {\n",
    "          'lr' : trial.suggest_loguniform('lr', 1e-4, 1e-3),\n",
    "          'optimizer': trial.suggest_categorical('optimizer',list(range(len(optimizers)))),\n",
    "          'gamma' : trial.suggest_uniform('gamma', .95, 1),\n",
    "          'init_v' : trial.suggest_loguniform('init_v', 3,6)\n",
    "        }\n",
    "        \n",
    "        global player\n",
    "        global optmodel\n",
    "        global epochs\n",
    "        optmodel =  Flappy()\n",
    "        optmodel.to(device)\n",
    "        \n",
    "        learning_epochs = optmodel.training_loop(nepochs, nn.SmoothL1Loss(), optimizers[cfg['optimizer']], lr=cfg['lr'], test_policy=10,\n",
    "                                                 gamma=cfg['gamma'], initial_value=cfg['init_v'],\n",
    "                                                 bad_state_penalty = 0, optunaprint=player, maxplayers = ntrials,\n",
    "                                                 optuna_epoch_limit = epochs, maxscore = 1000);\n",
    "        if learning_epochs < epochs:\n",
    "            epochs = learning_epochs\n",
    "        \n",
    "        player = player + 1\n",
    "        return learning_epochs\n",
    "    \n",
    "    study.optimize(optuna_train, n_trials=ntrials, callbacks=[callback])\n",
    "    \n",
    "    optuna_best_params = {\n",
    "        'lr'    : study.best_params['lr'],\n",
    "        'optimizer'   : optimizers[study.best_params['optimizer']],\n",
    "        'gamma' : study.best_params['gamma'],\n",
    "        'init_v' : study.best_params['init_v']\n",
    "    }\n",
    "    \n",
    "    return study, optuna_best_params, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf3a6b-4db5-4d35-9af9-3fe117c08026",
   "metadata": {},
   "source": [
    "### FlappyBird-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff15908-11f8-485a-9689-c47cbac7fc3b",
   "metadata": {},
   "source": [
    "![img1](https://raw.githubusercontent.com/Talendar/flappy-bird-gym/main/imgs/observations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652c2cc-2d68-4f3d-af5a-cec885199c1e",
   "metadata": {},
   "source": [
    "* ***observation space***: yields simple numerical information about the game's state as observations. The yielded attributes are the:\n",
    "  * horizontal distance to the next pipe;\n",
    "  * difference between the player's y position and the next hole's y position\n",
    "* ***actionspace***:\n",
    "  * tap: flutter\n",
    "  * do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c0b50427-a447-4f12-8d3b-f96e2cde7209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment informations:\n",
      "  observation space: Box(-inf, inf, (2,), float32)\n",
      "  action space     : Discrete(2)\n",
      "  nets input parameters : 2\n",
      "  nets output parameters: 2\n"
     ]
    }
   ],
   "source": [
    "player =  Flappy(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b12232f8-70c3-47a5-98c5-767a8682c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-07 22:42:38,002]\u001b[0m A new study created in memory with name: no-name-b894cc58-3e11-47b7-966f-59a96dfe3976\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8152e3e29ad9479abc98a67ce1f55ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-07 22:48:59,521]\u001b[0m Trial 0 finished with value: 754.0 and parameters: {'lr': 0.00027954149660166323, 'optimizer': 0, 'gamma': 0.9595566801424763, 'init_v': 4.38190024794958}. Best is trial 0 with value: 754.0.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea17411feb40469097f51c5d62bf4a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-07 22:53:51,505]\u001b[0m Trial 1 finished with value: 754.0 and parameters: {'lr': 0.00015526346001503786, 'optimizer': 1, 'gamma': 0.9576669573536944, 'init_v': 5.116800659400277}. Best is trial 0 with value: 754.0.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34feb4acc3814947944de221324da86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-07 22:56:39,619]\u001b[0m Trial 2 finished with value: 410.0 and parameters: {'lr': 0.0006064470182701977, 'optimizer': 0, 'gamma': 0.968036198211297, 'init_v': 5.7307419816483804}. Best is trial 2 with value: 410.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study2, optuna_best_params2, best_player2 = optuna_flappy(ntrials=3,nepochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1762beab-1f2e-46ee-bc31-7313dbe35f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 2948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2948, array([0.27430556, 0.13671875]))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_player2.play_a_game(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38256507-cb18-44e0-b1c4-befaa5125aff",
   "metadata": {},
   "source": [
    "### FlappyBird-rgb-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a8b9c-df0e-46d6-94d8-f3a50691c737",
   "metadata": {},
   "source": [
    "![img2](https://upload.wikimedia.org/wikipedia/it/thumb/c/c7/Flappy_Bird_gameplay.jpeg/135px-Flappy_Bird_gameplay.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4a419-5350-4879-ac5f-f782896ca41f",
   "metadata": {},
   "source": [
    "* ***observation space***: Whole RGB image, this means a 3 channel image of resolution 228x512 (228x512x3)\n",
    "* ***actionspace***:\n",
    "  * tap: flutter\n",
    "  * do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a27dab-7e66-4dc6-8183-7c9f96bc7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment informations:\n",
      "  observation space: Box(0.0, 255.0, (288, 512, 3), float32)\n",
      "  action space     : Discrete(2)\n",
      "  nets input parameters : 85050\n",
      "  nets output parameters: 2\n"
     ]
    }
   ],
   "source": [
    "player2 = Flappy(rgb = True, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2762d8-7e21-4aad-a63b-5fbe1c7b782c",
   "metadata": {},
   "source": [
    "### Optimizing encoding of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee5eef3-d04d-4086-814c-d8fb8b616522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(img, factor: int):\n",
    "    \"\"\" Perform max pooling with a (factor x factor) kernel\"\"\"\n",
    "    ds_img = np.full((img.shape[0] // factor, img.shape[1] // factor), -float('inf'), dtype=img.dtype)\n",
    "    np.maximum.at(ds_img, (np.arange(img.shape[0])[:, None] // factor, np.arange(img.shape[1]) // factor), img)\n",
    "    return ds_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a6f7e511-3b18-4a1f-b729-60bb9bce49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gym environmentif not rgb:\n",
    "env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "\n",
    "# Reset the environment and get the initial state\n",
    "state = env.reset()\n",
    "\n",
    "# Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "score = 0\n",
    "done = False\n",
    "# Go on until the pole falls off or the score reach 490\n",
    "while not done:\n",
    "    if score%20==0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    \n",
    "    # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "    next_state, _, done, info = env.step(action)\n",
    "    # Visually render the environment\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "    # Update the final score (+1 for each step)\n",
    "    score += 1\n",
    "    # Set the current state for the next iteration\n",
    "    state = next_state\n",
    "    # Check if the episode ended (the pole fell down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da41d5-fb9b-4885-b795-5226ce1917b9",
   "metadata": {},
   "source": [
    "#### RGB $\\to$ single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e3cca048-03a4-40af-88ef-5c85c0405bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 442368\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ndimage.rotate(np.flip(state,0), 270), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('./imgs/default_view.svg', format='svg')\n",
    "\n",
    "print('State dimension:', state.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3e1a2336-93b8-41b4-b91b-dd0f0ea94a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (288, 512, 3)\n",
      "State dimension: 147456\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Image size:',np.shape(state) )\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5))\n",
    "\n",
    "ch0 = state[:,:,0]\n",
    "ax[0].set_title('Channel 0')\n",
    "ax[0].imshow(ndimage.rotate(np.flip(ch0,0), 270), cmap='gray');\n",
    "ax[0].axis('off')\n",
    "ch1 = state[:,:,1]\n",
    "\n",
    "ax[1].set_title('Channel 1')\n",
    "ax[1].imshow(ndimage.rotate(np.flip(ch1,0), 270), cmap='gray');\n",
    "ax[1].axis('off')\n",
    "\n",
    "ch2 = state[:,:,2]\n",
    "ax[2].set_title('Channel 2')\n",
    "ax[2].imshow(ndimage.rotate(np.flip(ch2,0), 270), cmap='gray');\n",
    "ax[2].axis('off')\n",
    "plt.savefig('./imgs/channels.svg', format='svg')\n",
    "\n",
    "print('State dimension:', ch2.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fcb2e-1862-4b5e-b03c-14abd544aeed",
   "metadata": {},
   "source": [
    "#### Reducing size of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1a9cb907-77a0-4422-af76-ddd323c0b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2_red = ch2[70:280,0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "91b96f35-f4f7-4cc0-a16c-5534d87f36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel 2 cropped\n",
      "State dimension: 84000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\nChannel 2 cropped')\n",
    "plt.imshow(ndimage.rotate(np.flip(ch2_red,0), 270), cmap='gray');\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('./imgs/single_channel_cropped.svg', format='svg')\n",
    "\n",
    "print('State dimension:', ch2_red.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e564db-f218-4216-a945-1632473bfe07",
   "metadata": {},
   "source": [
    "#### Lowering resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "82d49333-a711-4c60-b6da-31b00c6f2b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 840\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAADnCAYAAADWz14pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD50lEQVR4nO3dMWoqURhAYecRsHATiWiha3AHrsL1xD3oCuzsLCwsrAUlEHQsQxorBQuZ1z143BsZ8RpNzvnKm5tkisPPzGScZEVRVMT1594HoPsyADgDgDMAOAOAezr3xdlsVvoS4fX1NVhrt9vRvbvdLlibTCbRvcPhsOwh/Aqj0Si6Xq1Wg7X5fB7dO51Og7WiKLLYXicAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQDc2ecBLpFl0T8368E5AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwALtmtYF808TM5AeAMAM4A4AwAzgDgfCAEzgkAZwBwBgBnAHDeCoZzAsAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwPnRMDgnAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAJbsV3Gq1rvp+byXfhxMAzgDgDADOAOCSnQTGHA6H6Hqe58Hafr+P7u31esHaYDC47sD0jxMAzgDgDADOAOAMAC7ZVUC32031o/SNnABwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHA3fSpYl6tWq9H17XYbrH1+fkb3NhqN0r/PCQBnAHAGAGcAcAYA51XAL7TZbErvdQLAGQCcAcAZAJwngQ/muz9i5wSAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwC4ZC+IGI/HpffudrtgbblcRvd+fHwEa4PBoPyB6SwnAJwBwBkAnAHA3fQtYYfDIbqe53mwtl6vo3trtVrSY9L/nABwBgBnAHAGAGcAcMmuAlarVbDWbreje19eXoK1r/7Jgbd9b8sJAGcAcAYAZwBwyU4Csyy76vuLokh0JLqEEwDOAOAMAM4A4M6eBHY6ndI/qN/vX30w+n5OADgDgDMAOAOAMwC4s1cBzWYzWHt/f4/u9VZuGl99xG673QZri8Uiuvft7S1YO51O0b1OADgDgDMAOAOAO3sSGHsgczab3exglEa9Xi+91wkAZwBwBgBnAHAGAHfTF0TocsfjMbr+/PwcrMXeoFapVCrT6bT073MCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQDcw7wgQvfhBIAzADgDgDMAuGQngX407GdyAsAZAJwBwBkAnAHAeSsYzgkAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHA+EALnBIAzADgDgDMAuMyTNzYnAJwBwBkAnAHAGQCcAcD9BQsYgP0fTa34AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ch2_red_lowres = max_pool(ch2_red, 10)\n",
    "plt.imshow(ndimage.rotate(np.flip(ch2_red_lowres,0), 270), cmap='gray');\n",
    "plt.axis('off')\n",
    "plt.savefig('./imgs/single_channel_cropped_maxpooled10.svg', format='svg')\n",
    "\n",
    "print('State dimension:', ch2_red_lowres.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ae0eb-d352-4178-b352-131abd83d462",
   "metadata": {},
   "source": [
    "#### Dynamic resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d22d2d65-a37c-4cfb-b965-5c79de0481fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 40)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2_red_lowres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d2b0a4dc-e77b-44e2-b16c-60ac3dae015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a2355232-b557-434d-9e96-76a34b2c669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 560\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(6,5))\n",
    "\n",
    "ax[0].imshow(ndimage.rotate(np.flip(ch2_red_lowres,0), 270),cmap='gray')\n",
    "ax[0].plot([16, 16], [0, 39], color='red')\n",
    "k = 16\n",
    "while(k<21):\n",
    "    ax[0].plot([k, k], [0, 39], color='red',alpha=0.2, lw = 3.48)\n",
    "    k = k + .5\n",
    "    \n",
    "ax[0].plot([12, 12], [0, 39], color='blue')\n",
    "k = 12\n",
    "while(k<16):\n",
    "    ax[0].plot([k, k], [0, 39], color='blue',alpha=0.2, lw = 3.4)\n",
    "    k = k + .5\n",
    "\n",
    "#ax[0].plot([2,2], [0,29], color = 'red')\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Pooled')\n",
    "#maxpool_col1 = ch2_red_lowres[:1,:].max(axis=0)\n",
    "maxpool_col2 = ch2_red_lowres[12:16,:].min(axis=0)\n",
    "maxpool_col3 = ch2_red_lowres[16:,:].min(axis=0)\n",
    "ch2_red_lowres_dym = np.vstack((np.vstack((ch2_red_lowres[:12,:], maxpool_col2 )), maxpool_col3)) \n",
    "ax[1].imshow(ndimage.rotate(np.flip(ch2_red_lowres_dym,0), 270), cmap='gray')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Reduced')\n",
    "\n",
    "plt.savefig('./imgs/single_channel_cropped_maxpooled10_reduced.svg', format='svg')\n",
    "\n",
    "print('State dimension:', ch2_red_lowres_dym.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3606c-531f-4fca-abad-c52810a22372",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ed24e772-4629-493e-affc-01164f371142",
   "metadata": {},
   "outputs": [],
   "source": [
    "player2 = Flappy(rgb = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4931ae87-5051-4c9c-9de6-d8273af45dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69f5e8a26e74f4888233c7032281560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3766/3687895648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3766/2804663121.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, max_epoch, loss_fn, optimizer_fn, initial_value, gamma, replay_memory_capacity, lr, reg_opt, target_net_update_steps, bad_state_penalty, test_policy, batch_size, min_samples_for_training, render, optunaprint, maxplayers, optuna_epoch_limit, maxscore)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;31m# Update the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_samples_for_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3766/2804663121.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, replay_mem, gamma, optimizer, loss_fn, batch_size)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Create tensors for each element of the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mstates\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mactions\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mrewards\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "player2.training_loop(2500, nn.SmoothL1Loss(), torch.optim.SGD, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "55e13467-4655-4cce-8eeb-1f0e352be80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 101\n"
     ]
    }
   ],
   "source": [
    "# Lets see once how it plays\n",
    "_, state = player2.play_a_game(debug = False, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3ac15-e1d7-4a3a-a993-30465fab50a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Resources:\n",
    "* [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
