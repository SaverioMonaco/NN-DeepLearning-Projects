{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd581c9-b9e4-4cb0-bd4d-dcc781d5d906",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "### A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "\n",
    "### Saverio Monaco\n",
    "##### MAT: 2012264\n",
    "\n",
    "# Homework 3 - Deep Reinforcement Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c01cf3-ee6a-4aa3-a1f9-5d22c6918a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### IMPORTS ####\n",
    "#################\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "from collections import deque # fixed size FIFO list\n",
    "\n",
    "# Deep Learning Stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import Monitor\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import base64\n",
    "\n",
    "# Other\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Automatic tuning tool\n",
    "import optuna\n",
    "\n",
    "# Second set\n",
    "import flappy_bird_gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2daba1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2684f8-fc99-43da-b1fc-37cf70a4eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a477b26c-d6d2-422a-b723-f6a38e6476ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742bba9d-45b6-4857-b692-3dc61d18fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### CLASSES ####\n",
    "#################\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''\n",
    "    To perform experience replay.\n",
    "    We will draw uniformly at random from the pool of stored sample to learn.\n",
    "    Thi avoids (temporal) correlation between consecutive learning instances.\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        ''' Initialize a deque with maximum capacity maxlen. '''\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        ''' Add a new sample to the deque, removes the oldest one if it is already full. '''\n",
    "        # Add the tuple (state, action, next_state, reward) to the queue\n",
    "        self.memory.append( (state, action, next_state, reward) )\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' Randomly select \"batch_size\" samples '''\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' Return the number of samples currently stored in the memory '''\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    ''' \n",
    "    Network for policy network and target network \n",
    "    state_space_dim:  (INPUT)  dimension of state space (e.g pixels in a image)\n",
    "    action_space_dim: (OUTPUT) dimension of action space (e.g go left, go right)\n",
    "    '''\n",
    "    def __init__(self, DQN_state_space_dim, DQN_action_space_dim):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.sdim = DQN_state_space_dim\n",
    "        self.adim = DQN_action_space_dim\n",
    "            \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.sdim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,self.adim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "#\n",
    "#          |---------> [Prediction Network (DQN)]--------\n",
    "#          |                |                            \\\n",
    "# [INPUT]--|                | Parameter update            \\___Loss\n",
    "#          |               \\/                            /\n",
    "#          |---------> [Target Network (DQN)]------------\n",
    "#\n",
    "class FullQNets(nn.Module):\n",
    "    ''' \n",
    "    Handles all the networks, environments, and others\n",
    "    '''\n",
    "    def __init__(self, envname):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.envname = envname\n",
    "        \n",
    "        tempenv = gym.make(self.envname) \n",
    "            \n",
    "        self.state_space_dim = tempenv.observation_space.shape[0]\n",
    "        \n",
    "        if self.envname == 'BipedalWalkerHardcore-v3':\n",
    "            self.action_space_dim = 4 # Box attribute in BipedalWalker are different\n",
    "        else:\n",
    "            self.action_space_dim = tempenv.action_space.n\n",
    "        \n",
    "        self.policy_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def choose_action_epsilon_greedy(self, state, epsilon):\n",
    "        self.policy_net.eval()\n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device) # Convert the state to tensor\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "\n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "        \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def choose_action_softmax(self, state, temperature):\n",
    "        self.policy_net.to(device)\n",
    "        if temperature < 0:\n",
    "            raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "        # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "        if temperature == 0:\n",
    "            return self.choose_action_epsilon_greedy(state, 0)\n",
    "    \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Apply softmax with temp\n",
    "        temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "        softmax_out = nn.functional.softmax(net_out / temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "        # Sample the action using softmax output as mass pdf\n",
    "        all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "        action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def update_step(self, replay_mem, gamma, optimizer, loss_fn, batch_size):        \n",
    "        self.policy_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        # Sample the data from the replay memory\n",
    "        batch = replay_mem.sample(batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Create tensors for each element of the batch\n",
    "        states      = torch.tensor([s[0] for s in batch], dtype=torch.float32, device=device)\n",
    "        actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64, device=device)\n",
    "        rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "        non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32, device=device) # the next state can be None if the game has ended\n",
    "        non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Compute all the Q values (forward pass)\n",
    "        self.policy_net.train()\n",
    "        q_values = self.policy_net(states)\n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            q_values_target = self.target_net(non_final_next_states)\n",
    "        next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def play_a_game(self, show = True, debug = False):\n",
    "        # Initialize the Gym environment\n",
    "        env = gym.make(self.envname) \n",
    "        \n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "        # Go on until the pole falls off or the score reach 490\n",
    "        while not done:\n",
    "            # Choose the best action (temperature 0)\n",
    "            action, q_values = self.choose_action_softmax(state, temperature=0)\n",
    "            if debug:\n",
    "                print('action', action)\n",
    "                print('state', state)\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Visually render the environment\n",
    "            if show:\n",
    "                env.render()\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += reward \n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "            # Check if the episode ended (the pole fell down)\n",
    "        # Print the final score\n",
    "        if show:\n",
    "            print(f\"SCORE: {score}\") \n",
    "        env.close()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def training_loop(self, max_epoch, loss_fn, optimizer_fn, initial_value=5, gamma = 0.97, \n",
    "                 replay_memory_capacity = 10000, lr = 1e-2, reg_opt = 1e-5, \n",
    "                 target_net_update_steps = 10, bad_state_penalty = 0, test_policy = False,\n",
    "                 batch_size = 128, min_samples_for_training = 1000, render=False, optunaprint = False, maxplayers = 0,\n",
    "                 optuna_epoch_limit = False):\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        gamma: gamma parameter for the long term reward\n",
    "        replay_memory_capacity: Replay memory capacity\n",
    "        lr: Optimizer learning rate\n",
    "        loss_fn: Loss function\n",
    "        optimizer_fn: Optimizer function\n",
    "        target_net_update_steps: Number of episodes to wait before updating the target network\n",
    "        batch_size: Number of samples to take from the replay memory for each update\n",
    "        bad_state_penalty: Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "        min_samples_for_training: Minimum samples in the replay memory to enable the training\n",
    "        test_policy: if True, for every target_net_update_steps steps, it will play 10 games following its policy (T=0)\n",
    "                     and if it obtains the maximum score (500) it will exit\n",
    "                     if is int > 0 it will do the same playing test_policy number of games\n",
    "        '''\n",
    "        env = gym.make(self.envname)\n",
    "        optimizer = optimizer_fn(self.policy_net.parameters(), lr=lr, weight_decay=reg_opt)\n",
    "        has_learned = False # Boolean variable, if test_policy == True and has obtained the max score on the test games\n",
    "                            # it will exit learning\n",
    "        # We compute the exponential decay in such a way the shape of the exploration \n",
    "        # profile does not depend on the number of iterations\n",
    "        exp_decay = np.exp(-np.log(initial_value) / max_epoch * 6) \n",
    "        exploration_profile = [initial_value * (exp_decay ** i) for i in range(max_epoch)]\n",
    "        \n",
    "        ### Initialize the replay memory\n",
    "        replay_mem = ReplayMemory(replay_memory_capacity)   \n",
    "        \n",
    "        env.seed(0)\n",
    "        policy_num = 0\n",
    "        progress = tqdm(exploration_profile)\n",
    "        #print('n iterations:', len(exploration_profile) )\n",
    "        for episode_num, tau in enumerate(progress):\n",
    "            # Reset the environment and get the initial state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "            score = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            # Go on until the pole falls off\n",
    "            while not done:\n",
    "                # Choose the action following the policy\n",
    "                action, q_values = self.choose_action_softmax(state, temperature=tau)\n",
    "                \n",
    "                # Apply the action and get the next state, the reward and a flag \"done\" \n",
    "                # that is True if the game is ended\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # We apply a (linear) penalty when the cart is far from center\n",
    "                pos_weight = 1\n",
    "                reward = reward - pos_weight * np.abs(state[0])\n",
    "                \n",
    "                # Update the final score (+1 for each step)\n",
    "                score += 1\n",
    "                \n",
    "                # Update the replay memory\n",
    "                replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "                # Update the network\n",
    "                if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                    optimizer = self.update_step(replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "                \n",
    "                if render:\n",
    "                    # Visually render the environment (disable to speed up the training)\n",
    "                    env.render()\n",
    "\n",
    "                # Set the current state for the next iteration\n",
    "                state = next_state\n",
    "\n",
    "            # Update the target network every target_net_update_steps episodes\n",
    "            if episode_num % target_net_update_steps == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "                \n",
    "                # If test_policy is True, we check every target_net_update_setps if the net has learned.+\n",
    "                # the net has learnet if it can obtain the maximum score 10 times in a row following its policy (with T=0)\n",
    "                # or test_policy times in a row if is int\n",
    "                if test_policy:\n",
    "                    policy_num = policy_num + 1\n",
    "                    if type(test_policy)==int:\n",
    "                        ngames = test_policy\n",
    "                    else:\n",
    "                        ngames = 10\n",
    "                        \n",
    "                    test_score = 0\n",
    "                    for game in range(ngames):\n",
    "                        test_score = test_score + self.play_a_game(show=False)\n",
    "                    test_score = test_score/ngames\n",
    "                    if not type(optunaprint)==int:\n",
    "                        print('Policy {:d} | Mean SCORE: {:.2f} '.format(policy_num,test_score) )\n",
    "                    if test_score >= 500:\n",
    "                        has_learned = True\n",
    "            # Print the final score\n",
    "            if optunaprint:\n",
    "                progress.set_description(\"{:d}/{:d} :: SCORE: {:d} - T: {:.4f}\".format(optunaprint,maxplayers, score,tau))\n",
    "            else:\n",
    "                progress.set_description(\"SCORE: {:d} - T: {:.4f}\".format(score,tau))\n",
    "            \n",
    "            if has_learned == True:\n",
    "                if not optunaprint:\n",
    "                    print('Learning completed at epoch: {:d}'.format(episode_num))\n",
    "                env.close()\n",
    "                return episode_num\n",
    "            \n",
    "            # For speeding up the training for the optuna gridsearch, we might want to kill the learning \n",
    "            # of models that are taking longer than the best one\n",
    "            if type(optuna_epoch_limit) == int:\n",
    "                if episode_num >= optuna_epoch_limit:\n",
    "                    env.close()\n",
    "                    return episode_num\n",
    "\n",
    "        env.close()\n",
    "        return max_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceacdc24-780d-4914-bf84-07e0393f0213",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b57674-066a-4182-9cd2-dd610a7e5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_qlearning(ntrials = 200, nepochs = 1000):\n",
    "    ''' Function for Optuna hyperparameter tuning '''\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='minimize')\n",
    "    \n",
    "    optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
    "    global player\n",
    "    player = 1\n",
    "    global epochs\n",
    "    epochs = nepochs\n",
    "    def callback(study, trial):\n",
    "        ''' Callback function, saves the best model. '''\n",
    "        global best_model\n",
    "        # If the best model result is the current one...\n",
    "        if study.best_trial == trial:\n",
    "            # Saves the best model to best_model\n",
    "            best_model = optmodel\n",
    "    \n",
    "    def optuna_train(trial):\n",
    "        ''' Main function for optuna. '''\n",
    "        cfg = {\n",
    "          'reg': trial.suggest_loguniform('reg',1e-5, 1e-3),\n",
    "          'lr' : trial.suggest_loguniform('lr', 1e-4, 1e-3),\n",
    "          'optimizer': trial.suggest_categorical('optimizer',list(range(len(optimizers)))),\n",
    "          'gamma' : trial.suggest_uniform('gamma', .95, 1),\n",
    "          #'penality' : trial.suggest_categorical('penality', [0]),\n",
    "          'init_v' : trial.suggest_loguniform('init_v', 1,10)\n",
    "        }\n",
    "        \n",
    "        global player\n",
    "        global optmodel\n",
    "        global epochs\n",
    "        optmodel =  FullQNets('CartPole-v1')\n",
    "        optmodel.to(device)\n",
    "        \n",
    "        learning_epochs = optmodel.training_loop(nepochs, nn.SmoothL1Loss(), optimizers[cfg['optimizer']], lr=cfg['lr'], test_policy=10,\n",
    "                                                 reg_opt = cfg['reg'], gamma=cfg['gamma'], initial_value=cfg['init_v'],\n",
    "                                                 bad_state_penalty = 0, optunaprint=player, maxplayers = ntrials,\n",
    "                                                 optuna_epoch_limit = epochs);\n",
    "        if learning_epochs < epochs:\n",
    "            epochs = learning_epochs\n",
    "        \n",
    "        player = player + 1\n",
    "        return learning_epochs\n",
    "    \n",
    "    study.optimize(optuna_train, n_trials=ntrials, callbacks=[callback])\n",
    "    \n",
    "    optuna_best_params = {\n",
    "        'reg'   : study.best_params['reg'],\n",
    "        'lr'    : study.best_params['lr'],\n",
    "        'optimizer'   : optimizers[study.best_params['optimizer']],\n",
    "        'gamma' : study.best_params['gamma'],\n",
    "        #'penality' : study.best_params['penality'],\n",
    "        'init_v' : study.best_params['init_v']\n",
    "    }\n",
    "    \n",
    "    return study, optuna_best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874df46e-11ce-425e-b955-d2f77c1b8540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study, optuna_best_params, best_player = optuna_qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ddbc5-625f-461c-8c28-fe63e1242022",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_player.play_a_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c4bdf-4f4c-4826-879a-1998aa259fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study, target_name='Episodes to learn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9724f1e-c0c2-4fef-9104-12e4e4d58664",
   "metadata": {},
   "source": [
    "## FlappyBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dd5227d-ee40-482b-a8e6-ec3d1ae915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### CLASSES ####\n",
    "#################\n",
    "    \n",
    "#\n",
    "#          |---------> [Prediction Network (DQN)]--------\n",
    "#          |                |                            \\\n",
    "# [INPUT]--|                | Parameter update            \\___Loss\n",
    "#          |               \\/                            /\n",
    "#          |---------> [Target Network (DQN)]------------\n",
    "#\n",
    "class Flappy(nn.Module):\n",
    "    ''' \n",
    "    Handles all the networks, environments, and others\n",
    "    '''\n",
    "    def __init__(self, rgb = False, verbose = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not rgb:\n",
    "            tempenv = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "            self.rgb = False\n",
    "            self.state_space_dim = tempenv.observation_space.shape[0]\n",
    "        else:\n",
    "            tempenv = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "            self.rgb = True\n",
    "            self.state_space_dim = 1\n",
    "            for i in range(len(tempenv.observation_space.shape) ):\n",
    "                self.state_space_dim = self.state_space_dim*tempenv.observation_space.shape[i]\n",
    "        \n",
    "        \n",
    "        self.action_space_dim = tempenv.action_space.n\n",
    "        \n",
    "        if verbose:\n",
    "            print('Environment informations:')\n",
    "            print('  observation space:', tempenv.observation_space)\n",
    "            print('  action space     :', tempenv.action_space)\n",
    "            print('  nets input parameters :', self.state_space_dim)\n",
    "            print('  nets output parameters:', self.action_space_dim)\n",
    "        \n",
    "        self.policy_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net = DQN(self.state_space_dim, self.action_space_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def choose_action_epsilon_greedy(self, state, epsilon):\n",
    "        self.policy_net.eval()\n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device) # Convert the state to tensor\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "\n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "        \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def choose_action_softmax(self, state, temperature):\n",
    "        self.policy_net.to(device)\n",
    "        if temperature < 0:\n",
    "            raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "        \n",
    "        # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "        if temperature == 0:\n",
    "            return self.choose_action_epsilon_greedy(state, 0)\n",
    "    \n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            net_out = self.policy_net(state)\n",
    "\n",
    "        # Apply softmax with temp\n",
    "        temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "        softmax_out = nn.functional.softmax(net_out / temperature, dim=0).cpu().numpy()\n",
    "                \n",
    "        # Sample the action using softmax output as mass pdf\n",
    "        all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "        action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n",
    "    \n",
    "        return action, net_out.cpu().numpy()\n",
    "    \n",
    "    def update_step(self, replay_mem, gamma, optimizer, loss_fn, batch_size):        \n",
    "        self.policy_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        # Sample the data from the replay memory\n",
    "        batch = replay_mem.sample(batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Create tensors for each element of the batch\n",
    "        states      = torch.tensor([s[0] for s in batch], dtype=torch.float32, device=device)\n",
    "        actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64, device=device)\n",
    "        rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "        # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "        non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32, device=device) # the next state can be None if the game has ended\n",
    "        non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Compute all the Q values (forward pass)\n",
    "        self.policy_net.train()\n",
    "        q_values = self.policy_net(states)\n",
    "        # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "        state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "        with torch.no_grad():\n",
    "            self.target_net.eval()\n",
    "            q_values_target = self.target_net(non_final_next_states)\n",
    "        next_state_max_q_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def play_a_game(self, show = False, debug = False):\n",
    "        # Initialize the Gym environmentif not rgb:\n",
    "        if self.rgb:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "        else:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "        \n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        done = False\n",
    "        # Go on until the pole falls off or the score reach 490\n",
    "        while not done:\n",
    "            # Choose the best action (temperature 0)\n",
    "            action, q_values = self.choose_action_softmax(state, temperature=0)\n",
    "            if debug:\n",
    "                print('---------------')\n",
    "                print('action', action)\n",
    "                print('state', state)\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Visually render the environment\n",
    "            if show:\n",
    "                env.render()\n",
    "                time.sleep(1 / 20)  # FPS\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += reward \n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "            # Check if the episode ended (the pole fell down)\n",
    "        # Print the final score\n",
    "        if show:\n",
    "            print(f\"SCORE: {score}\") \n",
    "        env.close()\n",
    "        \n",
    "        return score, state\n",
    "    \n",
    "    def training_loop(self, max_epoch, loss_fn, optimizer_fn, initial_value=5, gamma = 0.97, \n",
    "                 replay_memory_capacity = 10000, lr = 1e-2, reg_opt = 1e-5, \n",
    "                 target_net_update_steps = 10, bad_state_penalty = 0, test_policy = False,\n",
    "                 batch_size = 128, min_samples_for_training = 1000, render=False, optunaprint = False, maxplayers = 0,\n",
    "                 optuna_epoch_limit = False):\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        gamma: gamma parameter for the long term reward\n",
    "        replay_memory_capacity: Replay memory capacity\n",
    "        lr: Optimizer learning rate\n",
    "        loss_fn: Loss function\n",
    "        optimizer_fn: Optimizer function\n",
    "        target_net_update_steps: Number of episodes to wait before updating the target network\n",
    "        batch_size: Number of samples to take from the replay memory for each update\n",
    "        bad_state_penalty: Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n",
    "        min_samples_for_training: Minimum samples in the replay memory to enable the training\n",
    "        test_policy: if True, for every target_net_update_steps steps, it will play 10 games following its policy (T=0)\n",
    "                     and if it obtains the maximum score (500) it will exit\n",
    "                     if is int > 0 it will do the same playing test_policy number of games\n",
    "        '''\n",
    "        \n",
    "        if self.rgb:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")\n",
    "        else:\n",
    "            env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "            \n",
    "        optimizer = optimizer_fn(self.policy_net.parameters(), lr=lr, weight_decay=reg_opt)\n",
    "        has_learned = False # Boolean variable, if test_policy == True and has obtained the max score on the test games\n",
    "                            # it will exit learning\n",
    "        # We compute the exponential decay in such a way the shape of the exploration \n",
    "        # profile does not depend on the number of iterations\n",
    "        exp_decay = np.exp(-np.log(initial_value) / max_epoch * 6) \n",
    "        exploration_profile = [initial_value * (exp_decay ** i) for i in range(max_epoch)]\n",
    "        \n",
    "        ### Initialize the replay memory\n",
    "        replay_mem = ReplayMemory(replay_memory_capacity)   \n",
    "        \n",
    "        env.seed(0)\n",
    "        policy_num = 0\n",
    "        progress = tqdm(exploration_profile)\n",
    "        #print('n iterations:', len(exploration_profile) )\n",
    "        for episode_num, tau in enumerate(progress):\n",
    "            # Reset the environment and get the initial state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "            score = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            # Go on until the pole falls off\n",
    "            while not done:\n",
    "                # Choose the action following the policy\n",
    "                action, q_values = self.choose_action_softmax(state, temperature=tau)\n",
    "                \n",
    "                # Apply the action and get the next state, the reward and a flag \"done\" \n",
    "                # that is True if the game is ended\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # We apply a (linear) penalty when the cart is far from center\n",
    "                pos_weight = 5\n",
    "                reward = reward - pos_weight * np.abs(state[1])\n",
    "                \n",
    "                # Update the final score (+1 for each step)\n",
    "                score += 1\n",
    "                \n",
    "                # Update the replay memory\n",
    "                replay_mem.push(state, action, next_state, reward)\n",
    "\n",
    "                # Update the network\n",
    "                if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                    optimizer = self.update_step(replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "                \n",
    "                if render:\n",
    "                    # Visually render the environment (disable to speed up the training)\n",
    "                    env.render()\n",
    "                    time.sleep(1 / 30)  # FPS\n",
    "\n",
    "                # Set the current state for the next iteration\n",
    "                state = next_state\n",
    "\n",
    "            # Update the target network every target_net_update_steps episodes\n",
    "            if episode_num % target_net_update_steps == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n",
    "                \n",
    "                # If test_policy is True, we check every target_net_update_setps if the net has learned.+\n",
    "                # the net has learnet if it can obtain the maximum score 10 times in a row following its policy (with T=0)\n",
    "                # or test_policy times in a row if is int\n",
    "                if test_policy:\n",
    "                    policy_num = policy_num + 1\n",
    "                    if type(test_policy)==int:\n",
    "                        ngames = test_policy\n",
    "                    else:\n",
    "                        ngames = 10\n",
    "                        \n",
    "                    test_score = 0\n",
    "                    for game in range(ngames):\n",
    "                        test_score = test_score + self.play_a_game(show=False)\n",
    "                    test_score = test_score/ngames\n",
    "                    if not type(optunaprint)==int:\n",
    "                        print('Policy {:d} | Mean SCORE: {:.2f} '.format(policy_num,test_score) )\n",
    "                    if test_score >= 500:\n",
    "                        has_learned = True\n",
    "            # Print the final score\n",
    "            if optunaprint:\n",
    "                progress.set_description(\"{:d}/{:d} :: SCORE: {:d} - T: {:.4f}\".format(optunaprint,maxplayers, score,tau))\n",
    "            else:\n",
    "                progress.set_description(\"SCORE: {:d} - T: {:.4f}\".format(score,tau))\n",
    "            \n",
    "            if has_learned == True:\n",
    "                if not optunaprint:\n",
    "                    print('Learning completed at epoch: {:d}'.format(episode_num))\n",
    "                env.close()\n",
    "                return episode_num\n",
    "            \n",
    "            # For speeding up the training for the optuna gridsearch, we might want to kill the learning \n",
    "            # of models that are taking longer than the best one\n",
    "            if type(optuna_epoch_limit) == int:\n",
    "                if episode_num >= optuna_epoch_limit:\n",
    "                    env.close()\n",
    "                    return episode_num\n",
    "\n",
    "        env.close()\n",
    "        return max_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf3a6b-4db5-4d35-9af9-3fe117c08026",
   "metadata": {},
   "source": [
    "### FlappyBird-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff15908-11f8-485a-9689-c47cbac7fc3b",
   "metadata": {},
   "source": [
    "![img1](https://raw.githubusercontent.com/Talendar/flappy-bird-gym/main/imgs/observations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652c2cc-2d68-4f3d-af5a-cec885199c1e",
   "metadata": {},
   "source": [
    "* ***observation space***: yields simple numerical information about the game's state as observations. The yielded attributes are the:\n",
    "  * horizontal distance to the next pipe;\n",
    "  * difference between the player's y position and the next hole's y position\n",
    "* ***actionspace***:\n",
    "  * tap: flutter\n",
    "  * do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0b50427-a447-4f12-8d3b-f96e2cde7209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment informations:\n",
      "  observation space: Box(-inf, inf, (2,), float32)\n",
      "  action space     : Discrete(2)\n",
      "  nets input parameters : 2\n",
      "  nets output parameters: 2\n"
     ]
    }
   ],
   "source": [
    "player =  Flappy(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5ab27dc-bcea-449f-a85c-b80ea1024654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b6aa6f66a3418e9d36e62dbfdec7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5232/2515109324.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  states      = torch.tensor([s[0] for s in batch], dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5232/4192230850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5232/2515109324.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, max_epoch, loss_fn, optimizer_fn, initial_value, gamma, replay_memory_capacity, lr, reg_opt, target_net_update_steps, bad_state_penalty, test_policy, batch_size, min_samples_for_training, render, optunaprint, maxplayers, optuna_epoch_limit)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;31m# Update the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_samples_for_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5232/2515109324.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, replay_mem, gamma, optimizer, loss_fn, batch_size)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Apply gradient clipping (clip all the gradients greater than 2 for training stability)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "player.training_loop(2000, nn.SmoothL1Loss(), torch.optim.SGD, lr=1e-2, initial_value = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9065c6-08eb-4dfd-98ed-88f2226342df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test how good it can play, it will play 10 games and average the score:\n",
    "games = 10\n",
    "\n",
    "mean_score = 0\n",
    "for game in range(games):\n",
    "    score = player.play_a_game(show = False)\n",
    "    print('GAME {:d}: SCORE {:d}'.format(game+1,score) )\n",
    "    \n",
    "    mean_score = mean_score + score\n",
    "\n",
    "mean_score = mean_scores/games\n",
    "print('____________________')\n",
    "print('MEAN SCORE: {:2.f}'.format(mean_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa236974-7b75-4bf3-9db4-1e5711c8d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1690"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see once how it plays\n",
    "player.play_a_game(show = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38256507-cb18-44e0-b1c4-befaa5125aff",
   "metadata": {},
   "source": [
    "### FlappyBird-rgb-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a8b9c-df0e-46d6-94d8-f3a50691c737",
   "metadata": {},
   "source": [
    "![img2](https://upload.wikimedia.org/wikipedia/it/thumb/c/c7/Flappy_Bird_gameplay.jpeg/135px-Flappy_Bird_gameplay.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4a419-5350-4879-ac5f-f782896ca41f",
   "metadata": {},
   "source": [
    "* ***observation space***: Whole RGB image, this means a 3 channel image of resolution 228x512 (228x512x3)\n",
    "* ***actionspace***:\n",
    "  * tap: flutter\n",
    "  * do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58a27dab-7e66-4dc6-8183-7c9f96bc7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment informations:\n",
      "  observation space: Box(0.0, 255.0, (288, 512, 3), float32)\n",
      "  action space     : Discrete(2)\n",
      "  nets input parameters : 442368\n",
      "  nets output parameters: 2\n"
     ]
    }
   ],
   "source": [
    "player =  Flappy(rgb = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07022a7a-755b-4808-b3d6-8632f465c771",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (147456x3 and 442368x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5232/2861195584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_a_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5232/2515109324.py\u001b[0m in \u001b[0;36mplay_a_game\u001b[0;34m(self, show, debug)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Choose the best action (temperature 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5232/2515109324.py\u001b[0m in \u001b[0;36mchoose_action_softmax\u001b[0;34m(self, state, temperature)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action_epsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Evaluate the network output from the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5232/2515109324.py\u001b[0m in \u001b[0;36mchoose_action_epsilon_greedy\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert the state to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Get the best action (argmax of the network output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5232/1003880051.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (147456x3 and 442368x128)"
     ]
    }
   ],
   "source": [
    "score, state = player.play_a_game(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a1217d-fb67-4587-b724-48ea24a9f11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flappy' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5232/665753260.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Flappy' object has no attribute 'observation_space'"
     ]
    }
   ],
   "source": [
    "player.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f55f5f-3977-4c11-bf8f-bf308e340747",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempenv = flappy_bird_gym.make(\"FlappyBird-rgb-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "477d7087-497f-401d-902a-9eb0bbac6513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 512, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempenv.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f29883-c25d-46ef-93e4-c12ff754ca4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442368"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "288*512*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e546d5ca-bc9b-4228-93c5-2ed467e00c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempenv.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e426f96-0e46-4a32-b024-57d56136083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 1\n",
    "for i in range(len(tempenv.observation_space.shape) ):\n",
    "    dim = dim*tempenv.observation_space.shape[i]\n",
    "    \n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3ac15-e1d7-4a3a-a993-30465fab50a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Resources:\n",
    "* [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
