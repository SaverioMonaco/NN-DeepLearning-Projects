\documentclass[11pt,a4paper,twocolumn]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{tabularx, booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{pdfpages}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{svg}
%\usepackage{minted}
\svgsetup{inkscapeexe="C:/Program Files/Inkscape/bin/inkscape.exe"}

\usepackage{biblatex}
\addbibresource{bib1.bib}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% \sepline dopo \maketitle rende tutto piÃ¹ carino
\newcommand{\sepline}{\noindent\makebox[\linewidth]{\rule{\textwidth}{1.2pt}}}
\newcommand{\bsepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{1.2pt}}}
%\newcommand{\esepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.5pt}}}
\newcommand{\thinsepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.02pt}}}
\newcommand{\thinnersepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.01pt}}}

\author{Monaco Saverio - 2012264 \sepline \\Neural Networks and Deep Learning - Professor: A. Testolin}
\title{{\normalsize\textsc{Università degli studi di Padova}}\vspace{-.5cm} \\ \sepline\\ \textbf{Homework \#1
\\ Supervised Deep Learning}}

\begin{document}
	\maketitle
	\begin{abstract} As for the first homework, the tasks of classification and regression in a Supervised Deep Learning framework are investigated.\\ For regression, it is needed to effectively approximate a noisy unknown 1-dimensional function.\\ The classification task instead consists in building a Convolutional Neural Network for the FashionMNIST Dataset.\\
	For both tasks, more advanced techniques are further explored and compared.
	\end{abstract}
			%  present the simulation results
			\section{\textbf{Regression task}}
			% Describe quickly the task of regression
			The task of Regression in a Neural Network framework consist in approximating a scalar function:\vspace*{-.3cm}
			$$f:\mathbb{R}\to\mathbb{R}$$\vspace*{-.5cm}\\
			through the use of a Network. Usually, the deeper the network is, the more complex patterns and behaviours of the target function it can grasp, however it might also encounter overfitting on its training data.\\
			 For the current exercise, training and testing points are generated according to a theoretical and unknown function, plus some noise:\vspace*{-.1cm}
			$$\hat{y}=f(x) + noise$$\vspace*{-1cm}\\
			
			The data for this task is the following:\vspace*{-.5cm}
			\begin{figure}[h]
				\centering
				\includesvg[width=0.85\linewidth]{../imgs/regression/fulldataset}
			\end{figure}\\
			The points appear to be generated from a grade-5 polynomial function. It is worth noticing that the training dataset is not optimal, compared to the testing dataset: it is indeed noisier and it has missing values, namely for $x\in[-3,-1]$ and $x\in[2,3]$.\\
			The latter obstacle will be the most hard problem to overcome, since it requires the model to properly generalize on those two areas it has never been trained on.
			\subsection{\textbf{Methods}}
			% describe your model architectures and hyperparameters
			For the Regression task, it was implemented a Python class: \texttt{RegFC} to generate a Fully Connected Neural Network (FCNN) model with the following parameters:
			\begin{itemize}
				\item \texttt{Ni}: dimension of input vector (int);
				\item \texttt{No}: dimension of output vector (int);
				\item \texttt{Nhs}: number of neurons in each hidden layer (list of ints);
				\item \texttt{activation}: torch activation function;
				\item \texttt{dropout}: dropout rate after each hidden layer (float);
				\item \texttt{lr0}: initial Learning Rate of the optimizer (float);
				\item \texttt{reg\_term }: Regularization term of the optimzier (float);
			\end{itemize}
			Both \texttt{Ni} and \texttt{No} must be 1, being the model a 1-D Regressor, while the depth of the hidden layers was kept general: for example if \texttt{Nhs=[10,20,10]} it will create a 1-10-20-10-1 model.\medskip\\
			The loss function and the optimizer were instead kept fixed:
			\begin{itemize}
				\item Loss function: \texttt{nn.MSELoss()} (L2 norm);
				\item Optimizer: \texttt{optim.Adam()};
			\end{itemize}
			All models were evaluated using the Validation Error from \textit{k-fold cross validation} adoperating the sklearn function \texttt{sklearn.model\_selection.KFold}, for all models the number of folds was fixed to 5.\\
			To obtain optimal hyperparameters a Grid Search was manually implemented with nested for loops and keeping the model with the lowest validation error.
			\thinsepline\\
			\textbf{Parameters of Grid-Search:}\medskip\\
			\begin{tabular}{ll}
			\textbf{Lr}	& 1e-2, 1e-3 \\
			\textbf{Reg. terms}	& 1e-2, 1e-1 \\
			\textbf{H. layers} & [1000,1000], [1000,1000,1000] \\
			\textbf{Dropout} & 1e-2, 1e-1 \\
			\textbf{Act. fun.} & nn.Tanh(), nn.Sigmoid()
			\end{tabular}
			\thinsepline
			\subsection{\textbf{Results}}
				\subsubsection{Fully Connected Neural Network}
				The best parameters found using the Grid Search method were the following:
				\thinsepline\\
				\textbf{Best parameters:}\medskip\\
				\begin{tabular}{ll}
					\textbf{Lr}	& 1e-3 \\
					\textbf{Reg. terms}	& 1e-2 \\
					\textbf{H. layers} & [1000,1000,1000] \\
					\textbf{Dropout} & 1e-2 \\
					\textbf{Act. fun.} & nn.Tanh() \\
				\end{tabular}
				\thinnersepline\\
				\textbf{Epochs:} 1000\\
				\textbf{MSE on Test set:} 0.27585697\vspace*{-.2cm}\\
				\thinsepline\vspace*{-.7cm}\\
				\begin{figure}[h]
					\centering
					\includesvg[width=0.95\linewidth]{../imgs/regression/bestreg_performance}
					\caption{Performance of FCNN on both sets}
				\end{figure}\\
				Overall, one could say that the implementation of the Regressor is successfull, capturing the general tendency of the data, futhermore the model performs accordingly on the first patch of missing data (namely for $x\in[-3,-1]$), however it does not achieve the same goal on the other interval. This failure is mostly to blame on the training data being noisy and having missing patches on the most unpredictable parts throughout its range (namely two local maxima).
				\begin{figure}[h]
					\centering
					\includesvg[width=0.95\linewidth]{../imgs/regression/bestreg_losses}
					\caption{Train, Test and Validation loss curves}
				\end{figure}\\
				The figure above displays the learning curve for all 3 sets: Training-set, Validation-set e Test-set. The model seems to reach a plateau right after 100 epochs and then it shows swift bumps during the remaining epochs. Those spikes can be explained by the fact that the sets are limited (only 100 samples for training and test set).\\ In addition one can see that for the second half of the training process, the test error is slightly lower than the training error, this odd result is due to the fact that the test set is less noisy.\\ 
				\subsubsection{Polynomial model}
					Lastly, the FCNN was compared to a much simpler model: a \textit{Polynomial model} fitted to the training dataset using the method of least squares.\\
					In this particular case, in which data is clearly generated by adding noise to a polynomial function, a Polynomial model can be considered a viable alternative, however it restricts our hypothesis space to a much smaller space at it will not well generalize to other problems.\medskip\\
					The best model was found by the use of a grid search of the parameters:
					\begin{itemize}
						\item \texttt{grade:} maximum grade of the fitting polynomial function;
						\item \texttt{reg:} regularization term;
					\end{itemize}
					and applying the least square method on the training dataset and choosing the model with the lowest error on the test dataset.\newpage
					\begin{figure}[h]
						\centering
						\includesvg[width=0.95\linewidth]{../imgs/regression/polymodel}
					\end{figure}
					However, as one can see, the grade of the best model may be too high and it will not generalize well for points outside the range [-4,4].\medskip\\
					Being not a machine learning model, it is not wise to operate with training and test set, however in order to fairly compare this model to the FCNN it was preferred perform both Grid Searches equally.
	\section{\textbf{Classification task}}
		% Describe quickly the task of classification
		The objective of classification is to obtain a \textit{rule} that outputs the most probable label (belonging to discrete space $\mathcal{L}$) given a set of parameters X.\medskip\\
		The second task consists in implementing a Convolutional Neural Network to build a Classificator on the FashionMNIST dataset\cite{xiao2017fashion}. Each sample of data is a $28\times28$ grayscale image and its possible label is an integer between 0 and 9 (each number represent a type of clothing or accessories such as T-shirt/top, Trouser, Pullover).\vspace*{-.5cm}
		\begin{figure}[h]
			\centering
			\includesvg[width=0.95\linewidth]{../imgs/classification/fashionexamples}
			\caption{Samples of FashionMNIST}
		\end{figure}\\
		The size of the set for this task is broader, having 60,000 samples for the training set, and 10,000 samples for the test set.
		\subsection{\textbf{Methods}}
			% describe your model architectures and hyperparameters
			As for the previous task, the most optimal models were found using a custom made Grid Search and evaluating them using K-Fold Cross-Validation, with $K=5$.
			
			\subsection{\textbf{Results}}
			%  present the simulation results
			\subsubsection{Basic Convolutional Network}
			The structure of the model implemented is the following:
			\begin{itemize}
				\item \textit{First convolutional layer}
				\begin{itemize}
					\item in\_channels = 1;
					\item out\_channels = channels[0];
					\item kernel\_size = 5;
					\item stride = 1;
					\item padding = 2;
				\end{itemize} 
				\item A max pooling layer with kernel size 2
				\item \textit{Second convolutional layer}
				\begin{itemize}
					\item in\_channels = channels[0];
					\item out\_channels = channels[1];
					\item kernel\_size = 5;
					\item stride = 1;
					\item padding = 2;
				\end{itemize} 
				\item another max pooling layer with kernel size 2
				\item \textit{A final layer}
				\begin{itemize}
					\item input\_size = 7$\times$7$\times$channels[1]
					\item output\_size = 10
				\end{itemize}
			\end{itemize}
			As input during the initialization, one must specify the \texttt{channels} array (of two integers).\medskip\\
			For the Grid Search, different initial learning rates, regularization terms, channels arrays, dropout rates and activations were tried out, while the loss function was kept \texttt{nn.CrossEntropyLoss} and the optimizer was fixed \texttt{optim.Adam}
			\thinsepline\\
			\textbf{Best parameters}\medskip\\
			\begin{tabular}{ll}
				\textbf{Lr}	& 1e-3 \\
				\textbf{Reg. terms}	& 1e-4 \\
				\textbf{channels} & [6,12] \\
				\textbf{Dropout} & 1e-1 \\
				\textbf{Act. fun.} & nn.ReLU() \\
			\end{tabular}
			\thinnersepline\\
			\textbf{Epochs:} 100\\
			\textbf{Accuracy on Test set:} 89.81 \%\vspace*{-.2cm}\\
			\thinsepline\vspace*{-.7cm}\\
			\newpage
			\begin{figure}[h]
				\centering
				\includesvg[width=0.95\linewidth]{../imgs/classification/bestmodel_losses}
				\caption{First image: Loss curves on Validation and Training Set.\\ Second image: Accuracy of the model on the test set throughout its learning}
			\end{figure}
			\begin{figure}[h]
				\centering
				\includesvg[width=0.95\linewidth]{../imgs/classification/best_classificator_grid}
				\caption{Confusion matrix of the model on the test dataset}
			\end{figure}
			From the confusion matrix above, it is clear that the implementation of the classificator is overall convincing, however it does not particularly well distinguish between the classes "T-shirt/top" and "Shirt". This kind of behaviour was although expected since those two classes are aesthetically close.
			\subsubsection{Data augmentation}
			Augmenting the training data mainly helps a convolutional neural network obtain rotation invariance and  overall generalize better.
			Data augmentation was applied on the training set by applying the following transformations on the dataloader:\\
			%\begin{minted}[%
			%	xleftmargin=-7cm, fontsize=\small
			%	]{python}
			%	augmentation = 
			%	  transforms.Compose(
			%	   [transforms.ToTensor(),
			%	    transforms.RandomVerticalFlip(p=0.5),
			%	    transforms.RandomCrop(28,padding=2)])
				
			%\end{minted}
			Since having a larger training set required longer training time per epoch, it was opted not to perform a grid search and to stick with the most optimal parameters found with the model of the last section:
			\thinnersepline\\
			\textbf{Epochs:} 100\\
			\textbf{Accuracy on Test set:} 87.69 \%\vspace*{-.2cm}\\
			\thinnersepline\\
			In this case, data augmentation does not help the model reach an higher accuracy. This is most probably due to the fact that the images on the test set are well standardized, namely the clothes are perfectly in frame and not-rotated, hence, augmenting the data just misleads our model. \medskip\\
			\subsubsection{Finetuning a ResNet50}
			In addition, the pretrained model \texttt{resnet50} was modified to support grayscale images and two fully connected hidden layers were added at the end as finetuning.\\
			Due to the long periods of time for the training, no Grid Search was performed for this model either.
			\thinnersepline\\
			\textbf{lr0:} 1e-3\\
			\textbf{Loss:} nn.CrossEntropyLoss()\\
			\textbf{Optimizer:} optim.Adam() \\
			\textbf{Epochs:} 100 \\
			\textbf{Accuracy on Test set:} 91.57 \%\vspace*{-.2cm}\\
			\thinnersepline\\
			As for the main Classificator, the main source of error comes from not accurately distinguish the class "T-shirt/top" from "Shirt", however implementing a ResNet50 does improve the accuracy almost up to 92\%
	\printbibliography
	
	\onecolumn
	\section{\textbf{Appendix}}
		\subsection{Regression}
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/regression/bestreg_weights}
				\caption{Histogram of weights and biases of the best regressor found using Grid Search}
			\end{figure}
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/regression/bestreg_activations}
				\caption{Activation of the best regressor}
			\end{figure}
			\newpage
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/regression/untrained_weights}
				\caption{Histogram of weights and biases of a not trained model}
			\end{figure}
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/regression/untrained_activations}
				\caption{Activation of the same not trained model}
			\end{figure}
		\newpage\subsection{Classification}
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/classification/trainedlayer1}\\
				\includesvg[width=1\linewidth]{../imgs/classification/trainedlayer2}
				\caption{Filters of both convolutional layers}
			\end{figure}
			\begin{figure}[h]
				\centering
				\includesvg[width=1\linewidth]{../imgs/classification/trainedactivation1}\\
				\includesvg[width=1\linewidth]{../imgs/classification/trainedactivation2}
				\caption{Activation of both convolutional layers}
			\end{figure}
\end{document}