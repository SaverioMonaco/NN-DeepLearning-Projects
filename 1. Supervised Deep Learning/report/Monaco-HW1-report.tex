\documentclass[11pt,a4paper,twocolumn]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{tabularx, booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{pdfpages}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{svg}
%\svgsetup{inkscapeexe="C:/Program Files/Inkscape/bin/inkscape.exe"}

\usepackage{biblatex}
\addbibresource{bib1.bib}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% \sepline dopo \maketitle rende tutto piÃ¹ carino
\newcommand{\sepline}{\noindent\makebox[\linewidth]{\rule{\textwidth}{1.2pt}}}
\newcommand{\bsepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{1.2pt}}}
%\newcommand{\esepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.5pt}}}
\newcommand{\thinsepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.02pt}}}
\newcommand{\thinnersepline}{\noindent\makebox[\linewidth]{\rule{7.5cm}{0.01pt}}}

\author{Monaco Saverio - 2012264 \sepline \\Neural Networks and Deep Learning - Professor: A. Testolin}
\title{{\normalsize\textsc{Università degli studi di Padova}}\vspace{-.5cm} \\ \sepline\\ \textbf{Homework \#1
\\ Supervised Deep Learning}}

\begin{document}
	\maketitle
	\begin{abstract} As for the first homework, the main models of Supervised Deep Learning are investigated. For the regression task, it is needed to effectively approximate a noisy unknown\\ 1-dimensional function. The classification task instead consists in building a Convolutional Neural Network for the FashionMNIST Dataset.\\
	For both tasks, more advanced techniques are further explored and compared.
	\end{abstract}
			%  present the simulation results
			\section{\textbf{Regression task}}
			% Describe quickly the task of regression
			The task of Regression in a Neural Network framework consist in approximating a scalar function:
			$$f:\mathbb{R}\to\mathbb{R}$$
			through the use of a Network. Usually, the deeper the network is, the more complex patterns and behaviours of the target function it can grasp, however it might also encounter overfitting.\\
			 For the current exercise, training and testing points are generated according to a theoretical and unknown function, plus some noise:
			$$\hat{y}=f(x) + noise$$
			
			The data for the task is the following:\vspace*{-.5cm}
			\begin{figure}[h]
				\centering
				\includesvg[width=0.95\linewidth]{../imgs/regression/fulldataset}
			\end{figure}\\
			Data seems to be generated from a grade-5 polynomial function. It is worth noticing that the training dataset is not optimal, compared to the testing dataset: it is noisier and it has missing values, namely for $x\in[-3,-1]$ and $x\in[2,3]$..\\
			The latter obstacle will be the most hard problem to overcome, since it requires the model to properly generalize on those two areas it has never been trained on.
			\subsection{\textbf{Methods}}
			% describe your model architectures and hyperparameters
			For the Regression task, it was implemented a Python class: \texttt{RegFC} to generate a Fully Connected Neural Network (FCNN) model with the following parameters:
			\begin{itemize}
				\item \texttt{Ni}: dimension of input vector (int);
				\item \texttt{No}: dimension of output vector (int);
				\item \texttt{Nhs}: number of neurons in each hidden layer (list of int);
				\item \texttt{activation}: torch activation function;
				\item \texttt{dropout}: dropout rate after each hidden layer (float);
				\item \texttt{lr0}: initial Learning Rate for the optimizer (float);
				\item \texttt{reg\_term }: Regularization term for the optimzier (float);
			\end{itemize}
			Both \texttt{Ni} and \texttt{No} must be 1, being the model a 1-D Regressor, while the depth of the hidden layers was kept general: for example \texttt{Nhs = [10,20,10]} will create a 1-10-20-10-1 model.\medskip\\
			Loss function and optimizer were instead kept fixed:
			\begin{itemize}
				\item Loss function: \texttt{nn.MSELoss()} (L2 norm);
				\item Optimizer: \texttt{optim.Adam()};
			\end{itemize}
			The models were evaluated by the Validation Error implementing \textit{k-fold cross validation} using the skleanr function \texttt{sklearn.model\_selection.KFold}.
			To obtain optimal hyperparameters a Grid Search was manually implemented with nested for loops and keeping the model with the lowest validation error.
			\thinsepline\\
			\textbf{Parameters of Grid-Search}\medskip\\
			\begin{tabular}{ll}
			\textbf{Lr}	& 1e-2, 1e-3 \\
			\textbf{Reg. terms}	& 1e-2, 1e-1 \\
			\textbf{H. layers} & [1000,1000], [1000,1000,1000] \\
			\textbf{Dropout} & 1e-2, 1e-1 \\
			\textbf{Act. fun.} & nn.Tanh(), nn.Sigmoid()
			\end{tabular}
			\thinsepline
			\subsection{\textbf{Results}}
				\subsubsection{Fully Connected Neural Network}
				\thinsepline\\
				\textbf{Best parameters}\medskip\\
				\begin{tabular}{ll}
					\textbf{Lr}	& 1e-3 \\
					\textbf{Reg. terms}	& 1e-2 \\
					\textbf{H. layers} & [1000,1000,1000] \\
					\textbf{Dropout} & 1e-2 \\
					\textbf{Act. fun.} & nn.Tanh()
				\end{tabular}
				\thinnersepline\\
				\textbf{MSE on Test set:} 0.27585697\vspace*{-.2cm}\\
				\thinsepline\\
				\subsubsection{Polynomial model}
					Lastly, the FCNN was compared to a much simpler model: a \textit{Polynomial model} fitted to the training dataset fitted using the method of least squares.\\
					In this particular case, in which data is clearly generated by adding noise to a polynomial function, a Polynomial model can be considered as a viable alternative, however it restricts our hypothesis space to a much smaller space, meaning that it is not a good option in a general scenario.\medskip\\
					The best model was found by the use of a grid search of the parameters:
					\begin{itemize}
						\item \texttt{grade:} maximum grade of the fitting polynomial function;
						\item \texttt{reg:} regularization term;
					\end{itemize}
					applying the least square method on the training dataset and choosing the model with the lowest error on the test dataset:
					\vspace*{-.5cm}
					\begin{figure}[h]
						\centering
						\includesvg[width=0.95\linewidth]{../imgs/regression/polymodel}
					\end{figure}
					However, as one can see, the grade of the best model may be too high and it will not generalize well for points outside the range [-4,4].
	\section{\textbf{Classification task}}
		% Describe quickly the task of classification
		The objective of classification is to obtain a \textit{rule} that outputs the most probable label (belonging to discrete space $\mathcal{L}$) starting from a set of parameters $\mathcal{X}$.\\
		Solely for Classification, it is possible to define a particular metric that intuitively tells how well the model is performing:
		$$Accuracy: \frac{\text{\#samples} - \text{misclassified\_samples}}{\text{\#samples}}$$
		\subsection{\textbf{Methods}}
			% describe your model architectures and hyperparameters
		\subsection{\textbf{Results}}
			%  present the simulation results
			
	\section{\textbf{Conclusions}}
	%\printbibliography
	
	\newpage
	a
	\newpage
	\onecolumn
	\section{\textbf{Appendix}}
		
	
\end{document}